{
 "cells": [
  {
   "source": [
    "## Sagemaker container dummy example\n",
    "In this example is launched a dummy container using spot instances, to test what's the behavior of the aws Sagemaker with docker containers, and what's the behaviour of the container when it is terminated by aws due to lack of spot resources.</p>\n",
    "<p>In this example a dummy python script (into the container) performs similar actions that a normal training script with tensorflow or other framework should do, more specificaly:\n",
    "\n",
    "- Fake checkpoints are written in txt format in the folder `/opt/ml/checkpoints/`.\n",
    "\n",
    "- Fake tensorboard records are written every 20 seconds into the folder `/opt/ml/output/tensorboard/`, for check the real-time prensence into the s3 bucket folder  specified.\n",
    "\n",
    "- Furthermore the tree command is executed in the path `/opt/ml/` for inspect the folder structure created by sagemaker, and the result is stored in the `/opt/output/data/` folder as a txt file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06426f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import TensorBoardOutputConfig"
   ]
  },
  {
   "source": [
    "In this section is recovered the Sagemaker bucket generated by default from the service"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf426bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default sagemaker bucket name request \n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(\"bucket generated by sagemacker:\" + sagemaker_default_bucket)"
   ]
  },
  {
   "source": [
    "############################  JOB NAME  ####################################\n",
    "\n",
    "# job-name definition:\n",
    "# every multi-job session of training jobs is characterized by a base-job-name\n",
    "# the job-name on the contrary is the identifier of the single training job.\n",
    "# The job-name must be different for each training job and should be used to\n",
    "# divide the results of different training jobs into specific folders.\n",
    "job_name = 'test-11'"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Defining the s3 bucket for the training job\n",
    "In this section are defined all the nedeed variables that specify the paths to s3 buckets for the inputs and outputs data.\n",
    "\n",
    "It's worth spending a few words about the configuration of the TensorBoardOutputConfig, this path can be used for tensorboard data if you use tensorflow or for other types of files that is important for you to take out of the container during the training process. in this example we write some txt files filled with random chearacters in the `container_local_output_path` and this files became available in the `s3_output_path` relative to the TensorBoardOutputConfig in a few seconds. Unfortunately in our next example this configuration don't work, the file isn't load to s3 untill the call of fit_generator() of keras is finished maybe other configuration can allow to get realtime updates.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################   INPUTS  ####################################\n",
    "\n",
    "# repositroy ECR containing the docker image configured to be executed by Sagemaker\n",
    "ecr_container_uri = \"<your aws id>.dkr.ecr.<your aws region>.amazonaws.com/<your repo name:your repo tag>\"\n",
    "\n",
    "# s3 path containing the dataset needed for training the model\n",
    "dataset_bucket = \"s3://datsetsbucket/isic2018/test_dataset/\"\n",
    "\n",
    "# s3 path containing the model with pretrained weights, in the next example in this folder would be\n",
    "# stored the Mask R-CNN model trained on COCO. \n",
    "model_bucket = 's3://cermodelbucket'\n",
    "\n",
    "\n",
    "############################  OUTPUTS  ####################################\n",
    "\n",
    "# s3 path where are stored the results of the instance profiler and any other data saved during the training in the folder /opt/ml/output/data/\n",
    "output_path = f's3://{sagemaker_default_bucket}/output'\n",
    "\n",
    "# s3 path where are stored the checkpoints of the training proces\n",
    "checkpoints_path = f'{output_path}/{job_name}/checkpoints'\n",
    "\n",
    "# Definition of s3 target bucket folder for the tensorboard outputs and container folder where the tensorboard record must to be placed.\n",
    "# it's possible to place the tensorboard output in other places but sagemaker copy that records into '/opt/ml/output/tensorboard' so we decide to\n",
    "# put the records directly in there.\n",
    "# Note: in the path 's3://testtflogs/logs' the recors are divided into folders related to the job-name in this example the output of tensorboard\n",
    "# should be fine in 's3://testtflogs/logs/test-11'\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://testtflogs/logs',\n",
    "    container_local_output_path='/opt/ml/output/tensorboard'\n",
    ")\n"
   ]
  },
  {
   "source": [
    "In this section are recovered the execution role ARN associated to this notebook, that will be passed to the estimator for launching the training job, be sure to give permissions to use other buckets to this role, otherwise it will only be possible to use buckets starting with the sagemaker keyword, in this case the permission is needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the execution role of this instance of sagemaker notebook\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "# if you are lunching the notebook on your machine comment the rows above \n",
    "# and manualy put the role into the role variable,  the following is an example \n",
    "# retrive yours from IAM or from a notebook instance execution.\n",
    "\n",
    "# role = 'arn:aws:iam::<your aws id>:role/service-role/AmazonSageMaker-ExecutionRole-20210522T125188'"
   ]
  },
  {
   "source": [
    "In this section are defined the hyperparameters, this values are passed to the estimator definitions and would be reachable from the trainning script in the container as commandline arguments, or like environment variables whit this notation `SM_HP_{hyperparameter_name}`, es. `SM_HP_HP1` or `SM_HP_BATCH` in this case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e29ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters definition\n",
    "hyperparameters = {\n",
    "    'hp1': 'value_x',\n",
    "    'hp2': 314,\n",
    "    'hp3': 3.1415,\n",
    "    'batch': 7\n",
    "}"
   ]
  },
  {
   "source": [
    "## Setup the training job\n",
    "This is the key function of the script, in there are configured al the training job parameters, are passed all the path that was defined earlier, the hyperparameters and are defined many settings relative to the type of machine used for the job, and in witch mode should run.\n",
    "\n",
    "More specificaly we chose to run in spot mode (`use_spot_instances = True`), in this mode the cost of the training goes down from 50% to 80% depending on the instance type chosen and by the availability of the machine, this mode enable aws to sell at lower price unused compute capability in the cloud and can stop your application if someone need this machine in on-demand mode (without any discount).\n",
    "\n",
    "Whene you chose to run in spot mode two more variables should be set, `max_run` and `max_wait`, this variables specify how match time in seconds the container could run and the second specify in the case that it will be stoped by aws how much time the program should wait that spot instances became available again for restart your container, if one of the two limits are exceeded the training job will be terminated.\n",
    "\n",
    "For other info about the parameters of this function you can check the [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = sagemaker.estimator.Estimator(\n",
    "    image_uri    = ecr_container_uri, # container image \n",
    "    role         = role, # role of sagemaker notebook instance\n",
    "    instance_count = 1, # number of instance to launch\n",
    "    #train_instance_type=\"local\",  # if you want to use local mode\n",
    "    instance_type = 'ml.m5.large', # type of instance in witch place the training job\n",
    "    volume_size  = 50, # space in GB of the storage attached to the ec2 instance\n",
    "    max_run      = 10*3600, # max number of seconds of running for the job until the termination of the process\n",
    "    output_path  = output_path, # s3 path where will be placed the results of the profiler and the content of /opt/ml/output/data/ path as tar.gz file \n",
    "    #base_job_name=\"training-test\", # prefix for the trainng job name, if not specified generated automatically\n",
    "    hyperparameters = hyperparameters, # training job hyperparameters, parameters passed as command line arguments\n",
    "    model_uri    = model_bucket, # s3 path where to find the data needed for the start of the training job like the pretrained model,and that will be copied into the folder /opt/ml/inputs/data/model (sovrascrivibile da model_channel_name)\n",
    "    #model_channel_name = 'model', # name of the chanel where will be saved the data included in the path model_uri (/opt/ml/inputs/data/<model_channel_name>)\n",
    "    #metric_definitions = , # dict regexs for metrics extraction from stdout {\"<metric name>\":\"<regex for logs estraction>\", ...}\n",
    "    use_spot_instances = True, # flag for enabling the spot training\n",
    "    max_wait = 24*3600, # max time of waiting for spot instance to became available again\n",
    "    checkpoint_s3_uri = checkpoints_path, # s3 chekpoints target path\n",
    "    #checkpoint_local_path = '', # default: '/opt/ml/checkpoints'\n",
    "    #rules = ; # SageMaker Debugger rules\n",
    "    tensorboard_output_config = tensorboard_output_config # Tensorboard output configuration\n",
    "    #environment = {}, # dict usefull for setting more environment variables into the container\n",
    "    #max_retry_attempts =  # max number of try to restart the job if it's finish unespectedly \n",
    ")"
   ]
  },
  {
   "source": [
    "## Launching the training job\n",
    "In this section of the notebook the training-job start, using the `.fit()` method of estimator object.\n",
    "\n",
    "This method have the input parameter that could be defined as a dict with a key and a path to local storage or s3 bucket where are present files that we would download to the container, note that the number of path that could be specified are not limited and for each input path in the container into `/opt/ml/inputs/` is created one folder with the name equal to the passed key name and containing the data into the argument path. In this case we only passed one path to input param and we have as a result `/opt/ml/inputs/dataset/` into the container filled with the same data placed into `dataset_bucket ='s3://datsetsbucket/isic2018/test_dataset/'`, if you pass this dict with more values the result will be an `inputs/` folder populated with more subfolders and relative data.\n",
    "\n",
    "The `job_name` is a very important parameter this enable you to distinguish from different jobs launched simultaniously or in different moments and permit to distinguish the training job in the training job panel for this reason they can't have the same name, if you launch two training job with the same name the result is an error and the container dosen't start.\n",
    "\n",
    "Another note is relative to wait and logs parameters, this parameters enable you to watch the logs relative to the startup of the machine and to watch the training job machine logs outputs at the end of the training process (it's not possible to see the training job stdout in real-time). With this configuration the function fit don't terminate until the job isn't finished, but if you don't enable the wait parameter the job start and fit function terminate so you can launch other functions or other jobs,  so to see the logs the wait param should be true.\n",
    "\n",
    "If something is unclear you can check the relative [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24081b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test.fit(\n",
    "    inputs      = {\n",
    "        'dataset': dataset_bucket\n",
    "    },\n",
    "    job_name    = job_name,\n",
    "    wait        = True,\n",
    "    logs        = 'All'\n",
    ")"
   ]
  },
  {
   "source": [
    "## How to manage the spot instance interruption\n",
    "\n",
    "If you plan to use the spot instances for saving money, you shuld know that whene your training job is terminated before than it's done because aws reclaim their machine your data placed in the outputs folders like `/opt/ml/output/data/` or `/opt/ml/checkpoints/` will be saved into the s3 path specified. Whene the spot instances aviability came back sagemaker restart your training job automaticaly, but your program will be restarted, so the logic to restart the program from the last checkpoint is your responsability, fortunately it's not so hard to implement a script that restart from the last checkpoint. Whene the training job restart the checkpoint folder will be refilled with the contenet of s3 path specified at the first launch,  with the checkpoints saved before the interruption so the only procedure needed to restore the last checkpoint is to check if the folder `/opt/ml/checkpoints` is empty or not, if it isn't empty (and there isn't no errors in the paths definitions) you should read the checkpoints name, load the last checkpoint and restart the training from this epoch. \n",
    "\n",
    "## How to simulate the spot instance interruption before the job is done?\n",
    "\n",
    "The sigterm sended from aws to the container is the same generated by `docker stop -t 120`, so the container will stoped 2 minuts after aws reclaim the machine or after that you send the command of termination. To send to the container this command is necessary go into the sagemaker panel, open the side menu on the right you should see the \"training\" button click it, in the sub menu should be placed the button \"training process\", click it. now if your job is already started you should see the same job-name passed at the `.fit()` method, if you select it and go to \"operations\" (up at the right) there should be the option \"terminate\" or something similar, if you click it in 2-3 minutes the job will be terminated.\n",
    "\n",
    "The next step is to relaunch the job leaving everithing as is, so don't relaunch no notebook cells except the next cell after that the precedent cell was terminated. IN the next cell the training job was relaunched with the same estimator object configured erlier but this time we need to change the job-name otherwise otherwise we end up getting an error.\n",
    "\n",
    "note in our case the code restart and don't do anithing about the already existing \"fake\" checkpoints, but allow you to se the folder structure inside `opt/ml/` after the restart, so you can se where the data are placed whene the job restart, note that the folder tree is the first command launched whene the program start so there is anithing added by the script."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restarting the job unfinished.\n",
    "training_test.fit(\n",
    "    inputs      = {\n",
    "        'dataset': dataset_bucket\n",
    "    },\n",
    "    job_name    = job_name + \"-restart-test\",\n",
    "    wait        = True,\n",
    "    logs        = 'All'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd0b19f05f9342924ce7a5103d60a033f2d0ed3b1132f3eb0781eae98efaac103a9",
   "display_name": "Python 3.6.9 64-bit ('tf_1.14.0': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}