{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Sagemaker container lesion example\n",
    "In this example is launched a dummy container using spot instances, to test what's the behavior of the aws Sagemaker with docker containers, and what's the behaviour of the container whene it is terminated by aws due to lack of spot resources.</p>\n",
    "<p>In this example is simulated by a dummy python script (into the container) that performs similar actions that a normal training script with tensorflow or other framework should do, more specificaly:\n",
    "\n",
    "- Fake checkpoints are written in txt format in the folder `/opt/ml/checkpoints/`.\n",
    "\n",
    "- Fake tensorboard records are written every 20 seconds into the folder `/opt/ml/output/tensorboard/`, for check the real-time prensence into the s3 bucket folder  specified.\n",
    "\n",
    "- Furthermore the tree command is executed in the path `/opt/ml/` for inspect the folder structure created by sagemaker, and the result is stored in the `/opt/output/data/` folder as a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06426f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import TensorBoardOutputConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "In this section is recovered the Sagemaker bucket generated by default from the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf426bd0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bucket generated by sagemacker:sagemaker-eu-west-1-011827850615\n"
     ]
    }
   ],
   "source": [
    "# default sagemaker bucket name request \n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(\"bucket generated by sagemacker:\" + sagemaker_default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  JOB NAME  ####################################\n",
    "\n",
    "# job-name definition:\n",
    "# every multi-job session of training jobs is characterized by a base-job-name\n",
    "# the job-name on the contrary is the identifier of the single training job.\n",
    "# The job-name must be different for each training job and should be used to\n",
    "# divide the results of different training jobs into specific folders.\n",
    "job_name = 'cast-test-polish-6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Defining the s3 bucket for the training job\n",
    "In this section are defined all the nedeed variables that specify the paths to s3 buckets for the inputs and outputs data.\n",
    "\n",
    "It's worth spending a few words about the configuration of the TensorBoardOutputConfig, this path can be used for tensorboard data if you use tensorflow or for other types of files that is important for you to take out of the container during the training process. in this example we write some txt files filled with random chearacters in the `container_local_output_path` and this files became available in the `s3_output_path` relative to the TensorBoardOutputConfig in a few seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fc4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################   INPUTS  ####################################\n",
    "\n",
    "# repositroy ECR containing the docker image configured to be executed by Sagemaker\n",
    "# ecr_container_uri = \"<your aws id>.dkr.ecr.<your aws region>.amazonaws.com/<your repo name:your repo tag>\"\n",
    "ecr_container_uri = \"011827850615.dkr.ecr.eu-west-1.amazonaws.com/maskrcnn_repo_test:cast_2\"\n",
    "#ecr_container_uri = \"011827850615.dkr.ecr.eu-west-1.amazonaws.com/maskrcnn_repo_test:lesion_2\"\n",
    "\n",
    "# s3 path containing the dataset needed for training the model\n",
    "dataset_bucket = \"s3://datsetsbucket/cast_dataset_polish/\"\n",
    "\n",
    "# s3 path containing the model with pretrained weights, in the next example in this folder would be\n",
    "# stored the Mask R-CNN model trained on COCO. \n",
    "model_bucket = 's3://cermodelbucket'\n",
    "\n",
    "############################  OUTPUTS  ####################################\n",
    "\n",
    "# s3 path where are stored the results of the instance profiler and any other data saved during the training in the folder /opt/ml/output/data/\n",
    "output_path = f's3://{sagemaker_default_bucket}/output'\n",
    "\n",
    "# s3 path where are stored the checkpoints of the training proces\n",
    "checkpoints_path = f'{output_path}/{job_name}/checkpoints'\n",
    "\n",
    "# internal paths for checkpoints and tenorboard logs passed to the container as env variables\n",
    "user_defined_env_vars = {\"checkpoints\": \"/opt/ml/checkpoints\",\n",
    "                        \"tensorboard\": \"/opt/ml/output/tensorboard\"}\n",
    "\n",
    "# Definition of s3 target bucket folder for the tensorboard outputs and container folder where the tensorboard record must to be placed.\n",
    "# it's possible to place the tensorboard output in other places but sagemaker copy that records into '/opt/ml/output/tensorboard' so we decide to\n",
    "# put the records directly in there.\n",
    "# Note: in the path 's3://testtflogs/logs' the recors are divided into folders related to the job-name in this example the output of tensorboard\n",
    "# should be fine in 's3://testtflogs/logs/test-11'\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://testtflogs/logs',\n",
    "    container_local_output_path=user_defined_env_vars['tensorboard']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "In this section are recovered the execution role ARN associated to this notebook, that will be passed to the estimator for launching the training job, be sure to give permissions to use other buckets to this role, otherwise it will only be possible to use buckets starting with the sagemaker keyword, in this case the permission is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445b7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if you are running the code from jupiter\n",
    "# getting the execution role of this instance of sagemaker notebook\n",
    "# role = get_execution_role()\n",
    "# print(role)\n",
    "\n",
    "# if you are running the code from local\n",
    "role = 'arn:aws:iam::011827850615:role/service-role/AmazonSageMaker-ExecutionRole-20210522T125188'\n",
    "# role = 'arn:aws:iam::<your aws id>:role/service-role/<your role name>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "Definitions of regex for logs extraction from stdout of the training script in the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\"Name\": \"loss\",                    \"Regex\": r\"\\sloss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"rpn_class_loss\",          \"Regex\": r\"\\srpn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"rpn_bbox_loss\",           \"Regex\": r\"\\srpn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"mrcnn_class_loss\",        \"Regex\": r\"\\smrcnn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_loss\",                \"Regex\": r\"\\sval_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"mrcnn_mask_loss\",         \"Regex\": r\"\\smrcnn_mask_loss:\\s(\\d+.?\\d*)\\s\"},\n",
    "    {\"Name\": \"mrcnn_bbox_loss\",         \"Regex\": r\"\\smrcnn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_rpn_class_loss\",      \"Regex\": r\"\\sval_rpn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_rpn_bbox_loss\",       \"Regex\": r\"\\sval_rpn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_class_loss\",    \"Regex\": r\"\\sval_mrcnn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_bbox_loss\",     \"Regex\": r\"\\sval_mrcnn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_mask_loss\",     \"Regex\": r\"\\sval_mrcnn_mask_loss:\\s(\\d+.?\\d*)\"},\n",
    "    {\"Name\": \"ms/step\",                 \"Regex\": r\"\\s(\\d+)ms\\/step\"},\n",
    "    {\"Name\": \"epoch\",                   \"Regex\": r\"Epoch\\s(\\d+)\\/\\d*\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "In this section are defined the hyperparameters, this values are passed to the estimator definitions and would be reachable from the trainning script in the container as commandline arguments, or like environment variables whit this notation `SM_HP_{hyperparameter_name}`, es. `SM_HP_HP1` or `SM_HP_BATCH` in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e29ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters definition\n",
    "hyperparameters = {\n",
    "    \"NAME\": \"cast\", \n",
    "    \"GPU_COUNT\": 1, \n",
    "    \"IMAGES_PER_GPU\": 1,\n",
    "    \"AUG_PREST\": 1,\n",
    "    \"TRAIN_SEQ\": \"[\\\n",
    "        {\\\"epochs\\\": 250, \\\"layers\\\": \\\"all\\\", \\\"lr\\\": 0.0035 }\\\n",
    "    ]\",\n",
    "    \"LOSS_WEIGHTS\": \"{\\\n",
    "        \\\"rpn_class_loss\\\": 0.8,\\\n",
    "        \\\"rpn_bbox_loss\\\": 0.8,\\\n",
    "        \\\"mrcnn_class_loss\\\": 0.5,\\\n",
    "        \\\"mrcnn_bbox_loss\\\": 0.8,\\\n",
    "        \\\"mrcnn_mask_loss\\\": 0.7\\\n",
    "    }\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Setup the training job\n",
    "This is the key function of the script, in there are configured al the training job parameters, are passed all the path that was defined earlier, the hyperparameters and are defined many settings relative to the type of machine used for the job, and in witch mode should run.\n",
    "\n",
    "More specificaly we chose to run in spot mode (`use_spot_instances = True`), in this mode the cost of the training goes down from 50% to 80% depending on the instance type chosen and by the availability of the machine, this mode enable aws to sell at lower price unused compute capability in the cloud and can stop your application if someone need this machine in on-demand mode (without any discount).\n",
    "\n",
    "Whene you chose to run in spot mode two more variables should be set, `max_run` and `max_wait`, this variables specify how match time in seconds the container could run and the second specify in the case that it will be stoped by aws how much time the program should wait that spot instances became available again for restart your container, if one of the two limits are exceeded the training job will be terminated.\n",
    "\n",
    "For other info about the parameters of this function you can check the [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aaf3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = sagemaker.estimator.Estimator(\n",
    "    # container image \n",
    "    image_uri    = ecr_container_uri, \n",
    "    # role of sagemaker notebook instance\n",
    "    role         = role, \n",
    "    # number of instance to launch\n",
    "    instance_count = 1, \n",
    "    # if you want to use local mode\n",
    "    #train_instance_type=\"local\",  \n",
    "    # type of instance in which place the training job\n",
    "    # instance_type = 'ml.g4dn.xlarge',\n",
    "    instance_type = 'ml.g4dn.2xlarge',\n",
    "    # instance_type = 'ml.g4dn.12xlarge',\n",
    "    # instance_type = 'ml.p3.2xlarge',\n",
    "    # space in GB of the storage attached to the ec2 instance\n",
    "    #volume_size  = 50,\n",
    "    # max number of seconds of running for the job until the termination of the process\n",
    "    max_run      = 36*3600,\n",
    "    # s3 path where will be placed the results of the profiler and the content of /opt/ml/output/data/ path as tar.gz file \n",
    "    output_path  = output_path, \n",
    "    # prefix for the trainng job name, if not specified generated automatically\n",
    "    # base_job_name=\"training-test\", \n",
    "    # training job hyperparameters, parameters passed as command line arguments\n",
    "    hyperparameters = hyperparameters,\n",
    "    # list of tags for the job\n",
    "    tags = [{\"Key\": \"CER\", \"Value\": \"1\"},],\n",
    "    # s3 path where to find the data needed for the start of the training job like the pretrained model,and that will be copied into the folder /opt/ml/inputs/data/model (sovrascrivibile da model_channel_name)\n",
    "    model_uri    = model_bucket, \n",
    "    # name of the chanel where will be saved the data included in the path model_uri (/opt/ml/inputs/data/<model_channel_name>)\n",
    "    model_channel_name = 'model', \n",
    "    # dict regexs for metrics extraction from stdout {\"<metric name>\":\"<regex for logs estraction>\", ...}\n",
    "    metric_definitions = metrics, \n",
    "    # flag for enabling the spot training\n",
    "    use_spot_instances = True, \n",
    "    # max time of waiting for spot instance to became available again\n",
    "    max_wait = 48*3600, \n",
    "    # s3 chekpoints target path\n",
    "    checkpoint_s3_uri = checkpoints_path, \n",
    "    # default: '/opt/ml/checkpoints'\n",
    "    checkpoint_local_path = user_defined_env_vars['checkpoints'], \n",
    "    # SageMaker Debugger rules\n",
    "    #rules = ; \n",
    "    # Tensorboard output configuration\n",
    "    tensorboard_output_config = tensorboard_output_config,\n",
    "    # dict usefull for setting more environment variables into the container\n",
    "    environment = user_defined_env_vars\n",
    "    # max number of try to restart the job if it's finish unespectedly\n",
    "    #max_retry_attempts =   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Launching the training job\n",
    "In this section of the notebook the training-job start, using the `.fit()` method of estimator object.\n",
    "\n",
    "This method have the input parameter that could be defined as a dict with a key and a path to local storage or s3 bucket where are present files that we would download to the container, note that the number of path that could be specified are not limited and for each input path in the container into `/opt/ml/inputs/` is created one folder with the name equal to the passed key name and containing the data into the argument path. In this case we only passed one path to input param and we have as a result `/opt/ml/inputs/dataset/` into the container filled with the same data placed into `dataset_bucket ='s3://datsetsbucket/isic2018/test_dataset/'`, if you pass this dict with more values the result will be an `inputs/` folder populated with more subfolders and relative data.\n",
    "\n",
    "The `job_name` is a very important parameter this enable you to distinguish from different jobs launched simultaniously or in different moments and permit to distinguish the training job in the training job panel for this reason they can't have the same name, if you launch two training job with the same name the result is an error and the container dosen't start.\n",
    "\n",
    "Another note is relative to wait and logs parameters, this parameters enable you to watch the logs relative to the startup of the machine and to watch the training job machine logs outputs at the end of the training process (it's not possible to see the training job stdout in real-time). With this configuration the function fit don't terminate until the job isn't finished, but if you don't enable the wait parameter the job start and fit function terminate so you can launch other functions or other jobs,  so to see the logs the wait param should be true.\n",
    "\n",
    "If something is unclear you can check the relative [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24081b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-12 21:39:07 Starting - Starting the training job...\n",
      "2021-06-12 21:39:09 Starting - Launching requested ML instancesProfilerReport-1623533946: InProgress\n",
      "......\n",
      "2021-06-12 21:40:31 Starting - Preparing the instances for training......\n",
      "2021-06-12 21:41:43 Downloading - Downloading input data...\n",
      "2021-06-12 21:42:03 Training - Downloading the training image...........\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\n",
      "\u001b[0m\n",
      "\u001b[34mConfigurations:\u001b[0m\n",
      "\u001b[34mAUG_PREST                      1\u001b[0m\n",
      "\u001b[34mBACKBONE                       resnet101\u001b[0m\n",
      "\u001b[34mBACKBONE_STRIDES               [4, 8, 16, 32, 64]\u001b[0m\n",
      "\u001b[34mBATCH_SIZE                     1\u001b[0m\n",
      "\u001b[34mBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\u001b[0m\n",
      "\u001b[34mCOMPUTE_BACKBONE_SHAPE         None\u001b[0m\n",
      "\u001b[34mDETECTION_MAX_INSTANCES        100\u001b[0m\n",
      "\u001b[34mDETECTION_MIN_CONFIDENCE       0.7\u001b[0m\n",
      "\u001b[34mDETECTION_NMS_THRESHOLD        0.3\u001b[0m\n",
      "\u001b[34mDILATE_ITERATIONS_1            10\u001b[0m\n",
      "\u001b[34mDILATE_ITERATIONS_2            10\u001b[0m\n",
      "\u001b[34mDILATE_KERNEL                  [[1 1]\n",
      " [1 1]]\u001b[0m\n",
      "\u001b[34mDILATE_MASKS                   True\u001b[0m\n",
      "\u001b[34mDILATE_THERS_1                 500\u001b[0m\n",
      "\u001b[34mDILATE_THERS_2                 15000\u001b[0m\n",
      "\u001b[34mFPN_CLASSIF_FC_LAYERS_SIZE     1024\u001b[0m\n",
      "\u001b[34mGPU_COUNT                      1\u001b[0m\n",
      "\u001b[34mGRADIENT_CLIP_NORM             5.0\u001b[0m\n",
      "\u001b[34mIMAGES_PER_GPU                 1\u001b[0m\n",
      "\u001b[34mIMAGE_CHANNEL_COUNT            3\u001b[0m\n",
      "\u001b[34mIMAGE_MAX_DIM                  1024\u001b[0m\n",
      "\u001b[34mIMAGE_META_SIZE                17\u001b[0m\n",
      "\u001b[34mIMAGE_MIN_DIM                  800\u001b[0m\n",
      "\u001b[34mIMAGE_MIN_SCALE                0\u001b[0m\n",
      "\u001b[34mIMAGE_RESIZE_MODE              square\u001b[0m\n",
      "\u001b[34mIMAGE_SHAPE                    [1024 1024    3]\u001b[0m\n",
      "\u001b[34mLEARNING_MOMENTUM              0.9\u001b[0m\n",
      "\u001b[34mLEARNING_RATE                  0.001\u001b[0m\n",
      "\u001b[34mLOSS_WEIGHTS                   {'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 0.5, 'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'rpn_class_loss': 1.0}\u001b[0m\n",
      "\u001b[34mMASK_AUGMENTERS                ['Sequential', 'SomeOf', 'OneOf', 'Sometimes', 'Fliplr', 'Flipud', 'CropAndPad', 'Affine', 'PiecewiseAffine', 'ScaleX', 'ScaleY', 'TranslateX', 'TranslateY', 'Rotate', 'ShearX', 'ShearY', 'PiecewiseAffine', 'WithPolarWarping', 'PerspectiveTransform']\u001b[0m\n",
      "\u001b[34mMASK_POOL_SIZE                 14\u001b[0m\n",
      "\u001b[34mMASK_SHAPE                     [28, 28]\u001b[0m\n",
      "\u001b[34mMAX_GT_INSTANCES               100\u001b[0m\n",
      "\u001b[34mMEAN_PIXEL                     [143.75 143.75 143.75]\u001b[0m\n",
      "\u001b[34mMINI_MASK_SHAPE                (512, 512)\u001b[0m\n",
      "\u001b[34mNAME                           cast\u001b[0m\n",
      "\u001b[34mNUM_CLASSES                    5\u001b[0m\n",
      "\u001b[34mPOOL_SIZE                      7\u001b[0m\n",
      "\u001b[34mPOST_NMS_ROIS_INFERENCE        1000\u001b[0m\n",
      "\u001b[34mPOST_NMS_ROIS_TRAINING         2000\u001b[0m\n",
      "\u001b[34mPRE_NMS_LIMIT                  6000\u001b[0m\n",
      "\u001b[34mROI_POSITIVE_RATIO             0.33\u001b[0m\n",
      "\u001b[34mRPN_ANCHOR_RATIOS              [0.5, 1, 2]\u001b[0m\n",
      "\u001b[34mRPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\u001b[0m\n",
      "\u001b[34mRPN_ANCHOR_STRIDE              1\u001b[0m\n",
      "\u001b[34mRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\u001b[0m\n",
      "\u001b[34mRPN_NMS_THRESHOLD              0.7\u001b[0m\n",
      "\u001b[34mRPN_TRAIN_ANCHORS_PER_IMAGE    256\u001b[0m\n",
      "\u001b[34mSTEPS_PER_EPOCH                220\u001b[0m\n",
      "\u001b[34mTOP_DOWN_PYRAMID_SIZE          256\u001b[0m\n",
      "\u001b[34mTRAIN_BN                       False\u001b[0m\n",
      "\u001b[34mTRAIN_ROIS_PER_IMAGE           200\u001b[0m\n",
      "\u001b[34mTRAIN_SEQ                      [{'epochs': 150, 'layers': 'all', 'lr': 0.005}]\u001b[0m\n",
      "\u001b[34mUSE_MINI_MASK                  True\u001b[0m\n",
      "\u001b[34mUSE_RPN_ROIS                   True\u001b[0m\n",
      "\u001b[34mVALIDATION_STEPS               18\u001b[0m\n",
      "\u001b[34mWEIGHT_DECAY                   0.0001\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-12 21:44:13 Uploading - Uploading generated training model\n",
      "2021-06-12 21:44:13 Failed - Training job failed\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"cast_sagemaker.py\", line 296, in <module>\n",
      "    if os.listdir(model.checkpoints_dir_unique):\u001b[0m\n",
      "\u001b[34mFileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/checkpoints/cast'\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mE0612 21:44:07.923229 139744755414784 trainer.py:118] Reporting training FAILURE\u001b[0m\n",
      "\u001b[34mE0612 21:44:07.923612 139744755414784 trainer.py:120] framework error: \u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n",
      "    entrypoint()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 211, in main\n",
      "    train(env, framework.mapping.to_cmd_args(user_hyperparameters))\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 158, in train\n",
      "    runner=runner_type)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/entry_point.py\", line 94, in run\n",
      "    return _runner.get(runner, user_entry_point, args, env_vars, extra_opts).run(wait, capture_error)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 102, in run\n",
      "    capture_error=capture_error, cwd=_env.code_dir)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 48, in check_error\n",
      "    raise error_class(return_code=return_code, cmd=' '.join(cmd), output=stderr)\u001b[0m\n",
      "\u001b[34msagemaker_containers._errors.ExecuteUserScriptError: ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/usr/local/bin/python3.6 cast_sagemaker.py --AUG_PREST 1 --GPU_COUNT 1 --IMAGES_PER_GPU 1 --LOSS_WEIGHTS mrcnn_bbox_loss=1.0,mrcnn_class_loss=0.5,mrcnn_mask_loss=1.0,rpn_bbox_loss=1.0,rpn_class_loss=1.0 --NAME cast --TRAIN_SEQ [{'epochs': 150, 'layers': 'all', 'lr': 0.005}]\"\n",
      "\u001b[0m\n",
      "\u001b[34mExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/usr/local/bin/python3.6 cast_sagemaker.py --AUG_PREST 1 --GPU_COUNT 1 --IMAGES_PER_GPU 1 --LOSS_WEIGHTS mrcnn_bbox_loss=1.0,mrcnn_class_loss=0.5,mrcnn_mask_loss=1.0,rpn_bbox_loss=1.0,rpn_class_loss=1.0 --NAME cast --TRAIN_SEQ [{'epochs': 150, 'layers': 'all', 'lr': 0.005}]\"\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job cast-test-polish-4: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n    entrypoint()\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 211, in main\n    train(env, framework.mapping.to_cmd_args(user_hyperparameters))\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 158, in train\n    runner=runner_type)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/entry_point.py\", line 94, in run\n    return _runner.get(runner, user_entry_point, args, env_vars, extra_opts).run(wait, capture_error)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 102, in run\n    capture_error=capture_error, cwd=_env.code_dir)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 48, in check_error\n    raise error_class(return_code=return_code, cmd=' '.join(cmd), ou",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ae96d24339a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjob_name\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwait\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlogs\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;34m'All'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/.virtualenvs/tf_1.14.0/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_1.14.0/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_1.14.0/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3681\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3683\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf_1.14.0/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 ),\n\u001b[1;32m   3244\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             )\n\u001b[1;32m   3247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job cast-test-polish-4: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n    entrypoint()\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 211, in main\n    train(env, framework.mapping.to_cmd_args(user_hyperparameters))\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_tensorflow_container/training.py\", line 158, in train\n    runner=runner_type)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/entry_point.py\", line 94, in run\n    return _runner.get(runner, user_entry_point, args, env_vars, extra_opts).run(wait, capture_error)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 102, in run\n    capture_error=capture_error, cwd=_env.code_dir)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_containers/_process.py\", line 48, in check_error\n    raise error_class(return_code=return_code, cmd=' '.join(cmd), ou"
     ]
    }
   ],
   "source": [
    "training_test.fit(\n",
    "    inputs      = {\n",
    "        'dataset': dataset_bucket\n",
    "    },\n",
    "    job_name    = job_name,\n",
    "    wait        = True,\n",
    "    logs        = 'All'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd0136fa5ea572c9c81a9ea0ec8f43e2d55d45cb3cf87d50a9ac47eee5250b5e616",
   "display_name": "Python 3.6.9 64-bit ('tf_1.14.0': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}